{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Ernest/BITMEXMYSLAVE/bitmex-sandbox\n",
      "Installing collected packages: bitmex-sandbox\n",
      "  Found existing installation: bitmex-sandbox 0.0.1\n",
      "    Uninstalling bitmex-sandbox-0.0.1:\n",
      "      Successfully uninstalled bitmex-sandbox-0.0.1\n",
      "  Running setup.py develop for bitmex-sandbox\n",
      "Successfully installed bitmex-sandbox\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from IPython.display import clear_output\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\swagger_spec_validator\\validator20.py:53: SwaggerValidationWarning: Found \"$ref: #/definitions/UserPreferences\" with siblings that will be overwritten. See https://stackoverflow.com/a/48114924 for more information. (path #/definitions/User/properties/preferences)\n",
      "  ref_dict['$ref'], '/'.join(path),\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import bitmex_sandbox\n",
    "env = gym.make('bitbox-v0')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        state_space = env.observation_space.shape[0]*env.observation_space.shape[1]\n",
    "        action_space = env.action_space.n\n",
    "        num_hidden = 128\n",
    "\n",
    "        self.l1 = nn.Linear(state_space, num_hidden, bias=False)\n",
    "        self.l2 = nn.Linear(num_hidden, action_space, bias=False)\n",
    "\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Episode policy and reward history\n",
    "        self.episode_actions = torch.Tensor([])\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(state):\n",
    "    # Select an action (0 or 1) by running policy model\n",
    "    # and choosing based on the probabilities in state\n",
    "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "    state = torch.reshape(state, (-1,))\n",
    "    action_probs = policy(state)\n",
    "    distribution = Categorical(action_probs)\n",
    "    action = distribution.sample()\n",
    "    # Add log probability of our chosen action to our history\n",
    "    policy.episode_actions = torch.cat([\n",
    "        policy.episode_actions,\n",
    "        distribution.log_prob(action).reshape(1)\n",
    "    ])\n",
    "   \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.episode_rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / \\\n",
    "        (rewards.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.episode_actions, rewards).mul(-1), -1))\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item())\n",
    "    policy.reward_history.append(np.sum(policy.episode_rewards))\n",
    "    policy.reset()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes, profits=[]):\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment and record the starting state\n",
    "        state = env.reset()\n",
    "        for time in range(250):\n",
    "            action = predict(state)\n",
    "            # Uncomment to render the visual state in a window\n",
    "            # env.render()\n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "            # Save reward\n",
    "            policy.episode_rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        update_policy()\n",
    "        profit = env._get_stat()\n",
    "        profits.append(profit)\n",
    "        # Calculate score to determine when the environment has been solved\n",
    "        #profits.append(time)\n",
    "        #if episode % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('♂LASTEPISODE♂',episode,'/',episodes)\n",
    "    return profits\n",
    "        #print('♂LOSS♂',loss)\n",
    "        \n",
    "        #env.render()\n",
    "        #if mean_score > env.spec.reward_threshold:\n",
    "         #   print(\"Solved after {} episodes! Running average is now {}. Last episode ran to {} time steps.\"\n",
    "          #        .format(episode, mean_score, time))\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♂LASTEPISODE♂ 349 / 350\n"
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "profits = train(episodes = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.026890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.009811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  350.000000\n",
       "mean    -0.000322\n",
       "std      0.006118\n",
       "min     -0.026890\n",
       "25%     -0.000675\n",
       "50%      0.000661\n",
       "75%      0.002757\n",
       "max      0.009811"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "все что ниже неважно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорты\n",
    "import gym, bitmex\n",
    "from gym import spaces\n",
    "from gym.utils import seeding   \n",
    "import random, json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# константы\n",
    "MAX_ACCOUNT_BALANCE = 2147483647\n",
    "MAX_NUM_SHARES = 2147483647\n",
    "MAX_SHARE_PRICE = 13000\n",
    "MAX_STEPS = 20000\n",
    "INITIAL_ACCOUNT_BALANCE = 1000000000\n",
    "BIN_SIZE = '5m'\n",
    "SYMBOL = 'XBTUSD'\n",
    "OBSERV_WIN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SandboxSnd(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SandboxSnd, self).__init__()\n",
    "\n",
    "        self.df = self._get_df(BIN_SIZE,SYMBOL,MAX_STEPS)\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "\n",
    "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "        self.action_space = spaces.Box(\n",
    "            low = np.array([0, 0]), high = np.array([3, 1]), dtype=np.float16)\n",
    "\n",
    "        # Prices contains the OHCL values for the last five prices\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "    def _get_df(self, binSize, symbol, MAX_STEPS):\n",
    "        client = bitmex.bitmex(test=True)\n",
    "        out = []\n",
    "        for _ in range(0, MAX_STEPS, 1000):\n",
    "            page = client.Trade.Trade_getBucketed(\n",
    "                    binSize = binSize,\n",
    "                    symbol = symbol,\n",
    "                    count = 1000,\n",
    "                    start = _ ,\n",
    "                    reverse=True\n",
    "                ).result()[0]\n",
    "            out.extend(page)\n",
    "        out.reverse()\n",
    "        df = pd.DataFrame(out)\n",
    "        return df\n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'high'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'volume'].values / MAX_NUM_SHARES,\n",
    "        ])\n",
    "\n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "#######################\n",
    "    def _take_action(self, action):\n",
    "        # Set the current price to a random price within the time step\n",
    "        current_price = random.uniform(self.df.loc[self.current_step, \"open\"],\n",
    "                                       self.df.loc[self.current_step, \"close\"])\n",
    "        action_type = action[0]\n",
    "        amount = action[1]\n",
    "\n",
    "        if action_type < 1:\n",
    "            # Buy amount % of balance in shares\n",
    "            total_possible = int(self.balance / current_price)\n",
    "            shares_bought = int(total_possible * amount)\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
    "            self.shares_held += shares_bought\n",
    "\n",
    "        elif action_type < 2:\n",
    "            # Sell amount % of shares held\n",
    "            shares_sold = int(self.shares_held * amount)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step > MAX_STEPS-OBSERV_WIN-1:\n",
    "            self.current_step = 0\n",
    "\n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.balance * delay_modifier\n",
    "        done = self.net_worth <= 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = random.randint(0, MAX_STEPS-OBSERV_WIN-1)\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# импорты\n",
    "import gym, bitmex\n",
    "from gym import spaces\n",
    "from gym.utils import seeding   \n",
    "import random, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#################\n",
    "# константы\n",
    "MAX_ACCOUNT_BALANCE = 1\n",
    "MAX_NUM_SHARES = 100000\n",
    "MAX_SHARE_PRICE = 1/13000\n",
    "MAX_STEPS = 20000\n",
    "INITIAL_ACCOUNT_BALANCE = 0.01\n",
    "BIN_SIZE = '5m'\n",
    "SYMBOL = 'XBTUSD'\n",
    "OBSERV_WIN = 5\n",
    "#################\n",
    "class SandboxSnd(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "#################\n",
    "    def __init__(self):\n",
    "        super(SandboxSnd, self).__init__()\n",
    "\n",
    "        self.df = self._get_df(BIN_SIZE,SYMBOL,MAX_STEPS)\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "\n",
    "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "        self.action_space = spaces.Box(\n",
    "            low = np.array([0, 0]), high = np.array([3, 1]), dtype=np.float16)\n",
    "\n",
    "        # Prices contains the OHCL values for the last five prices\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "#################\n",
    "    def _get_df(self, binSize, symbol, MAX_STEPS):\n",
    "        client = bitmex.bitmex(test=True)\n",
    "        out = []\n",
    "        for _ in range(0, MAX_STEPS, 1000):\n",
    "            page = client.Trade.Trade_getBucketed(\n",
    "                    binSize = binSize,\n",
    "                    symbol = symbol,\n",
    "                    count = 1000,\n",
    "                    start = _ ,\n",
    "                    reverse=True\n",
    "                ).result()[0]\n",
    "            out.extend(page)\n",
    "        out.reverse()\n",
    "        df = pd.DataFrame(out)\n",
    "        return df\n",
    " #################\n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        frame = np.array([\n",
    "            (1/self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'open'].values) / MAX_SHARE_PRICE,\n",
    "            (1/self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'high'].values) / MAX_SHARE_PRICE,\n",
    "            (1/self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'low'].values) / MAX_SHARE_PRICE,\n",
    "            (1/self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'close'].values) / MAX_SHARE_PRICE,\n",
    "            (1/self.df.loc[self.current_step: self.current_step +\n",
    "                        OBSERV_WIN, 'volume'].values) / MAX_NUM_SHARES,\n",
    "        ])\n",
    "\n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "#######################\n",
    "    def _take_action(self, action):\n",
    "        # Set the current price to a random price within the time step\n",
    "        current_price = 1/random.uniform(self.df.loc[self.current_step,\"open\"],\n",
    "                                         self.df.loc[self.current_step, \"close\"])\n",
    "        action_type = action[0]\n",
    "        amount = action[1]\n",
    "        total_possible = int(self.balance / current_price)\n",
    "        shares_val = int(total_possible * amount)\n",
    "        if action_type < 1:\n",
    "            # Buy amount % of balance in shares\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_val * current_price\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (prev_cost + additional_cost) / (self.shares_held + shares_val)\n",
    "            self.shares_held += shares_val\n",
    "\n",
    "        elif action_type < 2:\n",
    "            # Sell amount % of shares held\n",
    "            self.balance += shares_val * current_price\n",
    "            self.shares_held -= shares_val\n",
    "            self.total_shares_sold += shares_val\n",
    "            self.total_sales_value += shares_val * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "#################\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step > MAX_STEPS-OBSERV_WIN-1:\n",
    "            self.current_step = 0\n",
    "\n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.net_worth * delay_modifier\n",
    "        done = self.net_worth <= 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "#################\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = random.randint(0, MAX_STEPS-OBSERV_WIN-1)\n",
    "        return self._next_observation()\n",
    "#################\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bitmex\n",
    "\n",
    "\n",
    "client = bitmex.bitmex(test=True)\n",
    "out = []\n",
    "for i in range(0, 5000, 1000):\n",
    "    page = client.Trade.Trade_getBucketed(\n",
    "            binSize = '1m',\n",
    "            symbol = 'XBTUSD',\n",
    "            count = 1000,\n",
    "            start = i,\n",
    "            reverse=True\n",
    "        ).result()[0]\n",
    "    out.extend(page)\n",
    "out.reverse()\n",
    "df = pd.DataFrame(out)\n",
    "\n",
    "\n",
    "MAX_ACCOUNT_BALANCE = 2147483647 # максимальный баланс\n",
    "MAX_NUM_SHARES = 2147483647 # максимальное число контрактов\n",
    "MAX_SHARE_PRICE = 5000 # максимальная цена контракта\n",
    "MAX_OPEN_POSITIONS = 5 # максимальное число открытых позиций\n",
    "MAX_STEPS = 20000 # число итераций в эпизоде\n",
    "\n",
    "INITIAL_ACCOUNT_BALANCE = 100000000000000 # баланс в начале\n",
    "\n",
    "\n",
    "class SandboxSnd(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SandboxSnd, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "\n",
    "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "        self.action_space = spaces.Box(low = np.array([0, 0]),\n",
    "                                       high = np.array([3, 1]), dtype=np.float16)\n",
    "        # Prices contains the OHCL values for the last five prices\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(6, 6),\n",
    "                                            dtype=np.float16)\n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'high'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'volume'].values / MAX_NUM_SHARES])\n",
    "\n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # Set the current price to a random price within the time step\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, \"open\"], self.df.loc[self.current_step, \"close\"])\n",
    "\n",
    "        action_type = action[0]\n",
    "        amount = action[1]\n",
    "\n",
    "        if action_type < 1:\n",
    "            # Buy amount % of balance in shares\n",
    "            total_possible = int(self.balance / current_price)\n",
    "            shares_bought = int(total_possible * amount)\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (\n",
    "                prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
    "            self.shares_held += shares_bought\n",
    "\n",
    "        elif action_type < 2:\n",
    "            # Sell amount % of shares held\n",
    "            shares_sold = int(self.shares_held * amount)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step > len(self.df.loc[:, 'open'].values) - 6:\n",
    "            self.current_step = 0\n",
    "\n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.balance * delay_modifier\n",
    "        done = self.net_worth <= 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = random.randint(\n",
    "            0, len(self.df.loc[:, 'open'].values) - 6)\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "#class SandboxSnd(gym.Env):\n",
    " #   metadata = {'render.modes': ['human']}\n",
    "  #  def __init__(self):\n",
    "   #     pass\n",
    "    #def step(self, action):\n",
    "     #   pass\n",
    "    #def reset(self):\n",
    "     #   pass\n",
    "    #def render(self, mode='human', close=False):\n",
    "     #   pass\n",
    "    \n",
    "    \n",
    "import random\n",
    "import json\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import bitmex\n",
    "\n",
    "client = bitmex.bitmex(test=True)\n",
    "out = []\n",
    "for i in range(0, 5000, 1000):\n",
    "    page = client.Trade.Trade_getBucketed(\n",
    "            binSize = '1m',\n",
    "            symbol = 'XBTUSD',\n",
    "            count = 1000,\n",
    "            start = i,\n",
    "            reverse=True\n",
    "        ).result()[0]\n",
    "    out.extend(page)\n",
    "out.reverse()\n",
    "df = pd.DataFrame(out)\n",
    "\n",
    "MAX_ACCOUNT_BALANCE = 2147483647\n",
    "MAX_NUM_SHARES = 2147483647\n",
    "MAX_SHARE_PRICE = 5000\n",
    "MAX_OPEN_POSITIONS = 5\n",
    "MAX_STEPS = 20000\n",
    "\n",
    "INITIAL_ACCOUNT_BALANCE = 100000000000000\n",
    "\n",
    "\n",
    "class SandboxSnd(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SandboxSnd, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.reward_range = (0, MAX_ACCOUNT_BALANCE)\n",
    "\n",
    "        # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float16)\n",
    "\n",
    "        # Prices contains the OHCL values for the last five prices\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Get the stock data points for the last 5 days and scale to between 0-1\n",
    "        frame = np.array([\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'open'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'high'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'low'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'close'].values / MAX_SHARE_PRICE,\n",
    "            self.df.loc[self.current_step: self.current_step +\n",
    "                        5, 'volume'].values / MAX_NUM_SHARES,\n",
    "        ])\n",
    "\n",
    "        # Append additional data and scale each value to between 0-1\n",
    "        obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "        ]], axis=0)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        # Set the current price to a random price within the time step\n",
    "        current_price = random.uniform(\n",
    "            self.df.loc[self.current_step, \"open\"], self.df.loc[self.current_step, \"close\"])\n",
    "\n",
    "        action_type = action[0]\n",
    "        amount = action[1]\n",
    "\n",
    "        if action_type < 1:\n",
    "            # Buy amount % of balance in shares\n",
    "            total_possible = int(self.balance / current_price)\n",
    "            shares_bought = int(total_possible * amount)\n",
    "            prev_cost = self.cost_basis * self.shares_held\n",
    "            additional_cost = shares_bought * current_price\n",
    "\n",
    "            self.balance -= additional_cost\n",
    "            self.cost_basis = (\n",
    "                prev_cost + additional_cost) / (self.shares_held + shares_bought)\n",
    "            self.shares_held += shares_bought\n",
    "\n",
    "        elif action_type < 2:\n",
    "            # Sell amount % of shares held\n",
    "            shares_sold = int(self.shares_held * amount)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_sold += shares_sold\n",
    "            self.total_sales_value += shares_sold * current_price\n",
    "\n",
    "        self.net_worth = self.balance + self.shares_held * current_price\n",
    "\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "\n",
    "        if self.shares_held == 0:\n",
    "            self.cost_basis = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        self._take_action(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step > len(self.df.loc[:, 'open'].values) - 6:\n",
    "            self.current_step = 0\n",
    "\n",
    "        delay_modifier = (self.current_step / MAX_STEPS)\n",
    "\n",
    "        reward = self.balance * delay_modifier\n",
    "        done = self.net_worth <= 0\n",
    "\n",
    "        obs = self._next_observation()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "        self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "\n",
    "        # Set the current step to a random point within the data frame\n",
    "        self.current_step = random.randint(\n",
    "            0, len(self.df.loc[:, 'open'].values) - 6)\n",
    "        return self._next_observation()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        # Render the environment to the screen\n",
    "        profit = self.net_worth - INITIAL_ACCOUNT_BALANCE\n",
    "\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(\n",
    "            f'Shares held: {self.shares_held} (Total sold: {self.total_shares_sold})')\n",
    "        print(\n",
    "            f'Avg cost for held shares: {self.cost_basis} (Total sales value: {self.total_sales_value})')\n",
    "        print(\n",
    "            f'Net worth: {self.net_worth} (Max net worth: {self.max_net_worth})')\n",
    "        print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
